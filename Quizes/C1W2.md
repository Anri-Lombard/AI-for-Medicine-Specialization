# Evaluating Machine Learning Models

1. What is the sensitivity and specificity of a pneumonia model that always outputs positive?  In other words, the models says that every patient has the disease.
   - sensitivity  = 1.0, specificity = 0.0

2. In some studies, you may have to compute the Positive predictive value (PPV) from the sensitivity, specificity and prevalence. 
Given a sensitivity = 0.9, specificity = 0.8, and prevalence = 0.2, what is the PPV (positive predictive value)? 
   - 0.53

3. If sensitivity = 0.9, specificity = 0.8, and prevalence = 0.2, then what is the accuracy? 
   - 0.82

4. What is the sensitivity and specificity of a model which randomly assigns a score between 0 and 1 to each example (with equal probability) if we use a threshold of 0.7? 
    - Sensitivity = 0.3, Specificity = 0.7

5. What is the PPV and sensitivity associated with the following confusion matrix?  
    - PPV = 0.3,  Sensitivity = 0.6

6. You have a model such that the lowest score for a positive example is higher than the maximum score for a negative example. What is its ROC AUC?  
   - 1.0

7. For every specificity, as we vary the threshold, the sensitivity of model 1 is at least as high as model 2. Which of the following must be true? 
   - The ROC of model 1 is at least as high as model 2

8. You want to measure the proportion of people with high blood pressure in a population. You sample 1000 people and find that 55% have high blood pressure with a 90% confidence interval of (50%, 60%). What is the correct interpretation of this result?  
    - If you repeated this sampling, the true proportion would be in the confidence interval about 90% of the time

9. One experiment calculates a confidence interval using 1000 samples, and the another computes it using 10000 samples. Which interval do you expect to be tighter (assume they use the normal approximation)?  
    - 10,000 samples
